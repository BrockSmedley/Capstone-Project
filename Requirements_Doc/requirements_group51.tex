\documentclass[letterpaper,10pt,serif,draftclsnofoot,onecolumn,compsoc,titlepage]{IEEEtran}

\usepackage{graphicx}                                        
\usepackage{amssymb}                                         
\usepackage{amsmath}                                         
\usepackage{amsthm}                                          
\usepackage{cite}
\usepackage{alltt}                                           
\usepackage{float}
\usepackage{color}
\usepackage{url}
\usepackage{pgfgantt}
\usepackage{rotating}

\usepackage{balance}
\usepackage[TABBOTCAP, tight]{subfigure}
\usepackage{enumitem}

\usepackage{geometry}
\geometry{margin=.75in}
\usepackage{hyperref}
%\usetikzlibrary{shapes, positioning, calc}
\usepackage{caption}
\usepackage{listings}
%\usepackage[utf8]{inputenc}
%pull in the necessary preamble matter for pygments output

%% The following metadata will show up in the PDF properties
\hypersetup{
   colorlinks = true,
   citecolor = black,
   linkcolor = black,
   urlcolor = black,
   pdfauthor = {Shu-Ping Chien, Brock Smedley, and W Keith Striby Jr},
   pdfkeywords = {CS461 "Senior Project" Requirements Document},
   pdftitle = {CS 461 Requirements Document},
   pdfsubject = {CS 461 Requirements Document},
   pdfpagemode = UseNone
}

\parindent = 0.0 in
\parskip = 0.1 in
\title{Requirements Document: Multi-Camera, System-on-Chip (SoC) Based, Real-Time Video Processing for UAS and VR/AR Applications}
\author{Group 51: Shu-Ping Chien, Brock Smedley, and W Keith Stirby Jr \\ 03 November 2017 \\ CS-461, Senior Software Engineering Project, Fall 2017}
\begin{document}
\begin{titlepage}
\maketitle
\begin{abstract}

This project aims to develop a system that creates a video feed from multiple cameras operating on different spectral bands. We will combine video feeds ranging from visible light to infrared and more. The resulting composite image will enable the user to see, via the feed, a clear image even in low-visibility conditions such as fog. We use methods in software operating on an NVIDIA Jetson TX1/2 device to create this composite image in near-real time.


\end{abstract}
\end{titlepage}
\newpage

\tableofcontents
\newpage

\section{Introduction}

\subsection{Purpose}

This software requirements specification is intended to define the requirements of the 
project of developing a multi-camera, multispectral image processing system, that 
operates on a System-on-Chip (SoC) at near-real-time, for use in ground and air based 
applications. Defined requirements will allow for a contract between us, the 
developers, and Rockwell Collins, our client, on what Rockwell Collins wants us to 
deliver in their desired software. This document is intended for review and reference 
by both the developers and the clients.\\

\subsection{Scope}

The product outlined in this requirements document will be the multi-camera, SoC based,
 real-time video processing for UAS and VR/AR applications. This product will need to 
 be able to generate a stitched video output from a multi-camera input. The product is 
 intended to help initialize our client's development of a cheaper alternative to a 
 product that is already offered to their customers.\\

The software products that will be produced include software for a stitched video output 
from the NVIDIA TX1/2, receiving the input from two visible band cameras. 
The video output is expected to be near real-time, and the latency from the camera 
input to the video output is expected to be improved upon throughout the project. Video 
output stretch goals is to have software that fuses the video output from the input of 
three, four, five, and six cameras, and have up to four infrared band inputs.\\

Output display stretch goals will be to incorporate IMU data, orientation tracking 
data, GPS data, and geolocate imagery. Two final stretch goals are packaging the 
hardware for flight, and interfacing the system to support the client's desired 
cameras for flight use.\\

The goal of the software is to contribute to a project that will assist pilots during 
low visibility conditions during the day, night, and inclement weather for all phases 
of flight. The video input from infrared and visible band cameras combined with 
on-board sensor input, and databases will enhance a pilot's vision for a UAS.\\

\subsection{Definitions, Acronyms, Abbreviations}

\subsubsection{Definitions}

\begin{description}
	\item[geolocate imagery] - 
	\item[low visibility] - 
	\item[multiple cameras] - At least two cameras, but a maximum of six cameras for 
	video input.
	\item[Near real-time] -
	\item[NVIDIA TX1/2] - NVIDIA GPUs, the Jetson TX1 or the Jetson TX2.
	\item[spectral bands] - 
\end{description}

\subsubsection{Acronyms}

\begin{tabular}{|l|l|}
\hline
\textbf{Term} & \textbf{Acronym}\\
\hline
Augmented Reality & AR\\
\hline
Camera Serial Interface & CSI\\
\hline
Enhanced Vision System & EVS\\
\hline
Global Positioning System & GPS\\
\hline
Graphic Processing Unit & GPU\\
\hline
Image Signal Processors & ISP\\
\hline
Inertial Measurement Unit & IMU\\
\hline
Head-up Display & HUD\\
\hline
System-on-chip & SoC\\
\hline
System-on-module & SOM\\
\hline
Size, weight, power and cost & SWaP-C\\
\hline
Three Dimensional & 3D\\
\hline
Two Dimensional & 2D\\
\hline
Video Input & VI\\
\hline
Unmanned Aerial Vehicle & UAV\\
\hline
Unmanned Aircraft System & UAS\\
\hline
Virtual Reality & VR\\
\hline
\end{tabular}

\subsubsection{Abbreviations}

...

\subsection{References}

...

\subsection{Overview}

This project aims to create a device that is capable of combining the video input from 
two or more cameras and produce and output at near real-time. Our proposed solution 
will use an NVIDIA Jetson device, which we will use for its integrated GPU.\\

We need this GPU to combine the images from multiple cameras. The end goal is to have 
a system that uses the input from multiple cameras that operate on the infrared and 
visible light spectral bands. By using these spectral bands, we should be able to 
produce an image that can be used to see in low-visibility situations, such as landing 
a UAV in fog.\\

The images we produce will be 2D representations of our collective image captures. In 
other words, we do not aim to create a 3D image or a dynamic focus image. This is 
certainly possible when using multiple cameras, but we simply aim to use multiple 
cameras on different spectral bands to create one image of one subject that is the 
combination of all images captured by the cameras.\\


\section{Overall Description}

\subsection{Product Perspective}

The system will be self-contained and consists of three parts: one NVIDIA TX1/2, 
one CSI carrier board, and at least two cameras. The cameras connect to the CSI board, 
which is connected to the NVIDIA TX1/2. The NVIDIA TX1/2 is responsible for decoding 
the serial data retrieved by the CSI board from the cameras, and is then be used to 
execute the software for image processing and combining images from multiple cameras.\\

\begin{figure}[H]
	\centering
	\label{fig:CopyOnWriteBefore}
	\includegraphics[width=10cm]{images/diagram.eps}
	\caption{Product Block Diagram \label{overflow}}
\end{figure}

\subsection{Product Functions}

The basic functionality of the product will be to produce stitched images on a video 
output that is provided by multiple camera inputs capable of sensing visible and 
infrared spectral bands. These images will be relayed in near real-time so that it 
can be used as a video feed for the pilot of a UAS during low visibility flight 
conditions. \\

A functional stretch goal for video output provided by camera input is to fuse the 
input from the visible and spectral bands, which will overlay the two types of output 
and enhance the vision for a UAS pilot. \\

Output display functional stretch goals will be to provide indications from IMU data, 
orientation tracking data, GPS data, and geolocate imagery and have them displayed 
with the video output provided by the camera input. \\

\subsection{Constraints}

The system must operate in near real-time. In other words, the camera feed(s) must be 
processed quickly enough for the user to make snap decisions based on the feed. The 
NVIDIA board should process each frame before the next one arrives to be processed. 
If weâ€™re recording at 30 frames per second (fps), then each output frame should be 
processed in less than 1/30 of a second.\\

\subsection{Assumptions \& Dependencies}


Software will be implemented on an NVIDIA TX1/2 device. Software will be deployed to the device with NVIDIA Jetpack software running on an Ubuntu machine. Adequate power supplies are required; they should meet the NVIDIA TX1/2 system requirements. All cameras in use should be aimed at same subject, capturing approximately the same image. Each camera should work independently of the system; if one fails, the others will still operate.

\section{Specific Requirements}
THIS NEEDS TO BE IN PARAGRAPH FORM, WITH SECTIONS FROM THE IEEE STD 830-1998\\
\subsection{Hardware Specifications}
\subsubsection{NVIDIA Jetson TX1/2}
	The NVIDIA TX1/2 will capture images through CSI interface and transfer data into GPU, which is used to combine the images from multiple cameras.\\ 
	The development environment of NVIDIA TX1/2 is available on Ubuntu system, and the module supports softwares for image processing.\\
	The NVIDIA TX1/2 is expected to require adequate power less than 7.5 watts.\\

\subsubsection{CSI Board}
	The carrier board should contain CSI interface, which is used to transfer camera input from up to six cameras to computer.\\
	The CSI board will provide output for a computer with pixel data and signals that can be used for subsequent image processing.\\
	The CSI board should support NVIDIA TX1/2, and the output format of CSI is depending on what cameras we chose.\\

\subsubsection{Cameras}
	The cameras with CSI interface are required in order to transfer image data to computer.\\
	The transfer rate is expected to operate in near-real-time, and the output format from cameras will be accepted by NVIDIA TX1/2.\\
	The cameras should be able to capture images from different spectral channels which includes Infrared, Ultraviolet, and visible light.\\
\subsection{Software Specifications}
	The software is expected to transfer input images into corresponding format for GPU.\\
	The software should be able to fuse images of infrared, ultraviolet, and visible light  and produce 2D video output, and the process should be completed before next input transferred in.\\


\section{Development Schedule}
	\begin{ganttchart}
    	[hgrid, x unit=0.77mm, y unit chart=9.0mm, title label font=\normalsize, time slot format=isodate]
    	{2017-11-01}{2018-05-31}
    	\gantttitlecalendar{year, month=name}\\
    	\ganttbar{Task 1}{2017-11-01}{2017-11-30}\\
    	\ganttbar{Task 2}{2017-11-15}{2018-01-06}\\
    	\ganttbar{Task 3}{2018-01-01}{2018-01-31}\\
    	\ganttbar{Task 4}{2018-01-15}{2018-02-28}\\
    	\ganttbar{Task 5}{2018-03-01}{2018-03-31}\\
    	\ganttbar{Task 6}{2018-04-01}{2018-04-30}\\
    	\ganttbar{Task 7}{2018-05-01}{2018-05-31}\\
	\end{ganttchart}
\begin{center}
	Fig. 2: Project Schedule
\end{center}	

\subsection{Development Schedule Tasks}
Task 1: Have hardware procured and assembled.\\
Task 2: Produce a tiled video output from the input of six cameras.\\
Task 3: Produce stitched video output from the input of two and three cameras, 
and have latency estimates produced.\\
Task 4: Produce a dual stitched video output that is combined into a fused 
five-camera output (stretch goal).\\
Task 5: Incorporate IMU data, orientation tracking data, GPS data, and 
geolocate imagery into the video output (stretch goals).\\
Task 6: Package the system hardware for flight (stretch goal).\\
Task 7: Produce a software interface for the system to accomodate higher 
quality cameras (stretch goal).\\

\end{document}
