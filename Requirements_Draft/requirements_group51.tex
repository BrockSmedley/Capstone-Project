\documentclass[]{report}


% Title Page
\title{Capstone Project Requirements}
\author{Keith Striby, Shu-Ping Chien, Brock Smedley}


\begin{document}
\maketitle

\section{Introduction}

\subsection{Purpose}
This software requirements specification is intended to define the requirements of the project of developing a multi-camera, multispectral image processing system, that operates on a System-on-Chip (SoC) at near-real-time, for use in ground and air based applications. Defined requirements will allow for a contract between us, the developers, and Rockwell Collins, our client, on what Rockwell Collins wants us to deliver in their desired software. This document is intended for review and reference by both the developers and the clients.

\subsection{Scope}
The product outlined in this requirements document will be the multi-camera, SoC based, real-time video processing for UAS and VR/AR applications. This product will need to be able to generate a fused video output from a multi-camera input. The product is intended to help initialize our client’s development of a cheaper alternative to a product that is already offered for their customers.

The software products that will be produced include software for a fused video output from the NVIDIA Jetson TX1 or TX2, receiving the input from two visible band cameras. The video output is expected to be near-real-time. Video input stretch goals is to have software that fuses the video output from the input of three, four, five, and six cameras, and have up to four infrared band inputs. (Do we mention hardware requirements for SWaP-C?)

The goal of the software is to contribute to a project that will assist pilots during low visibility conditions during the day, night, and inclement weather for all phases of flight. The video input from infrared and visible band cameras combined with on-board sensor input, and databases will enhance a pilot vision for an Unmanned Aerial Vehicle (UAV) and potentially manned, correct? (specify helicopter and/or airplane?)

\subsection{Technical terms}
...
\subsubsection{Definitions}
...
\subsubsection{Acronyms}
MAKE THIS INTO A TABLE

Term
Acronym
CSI
Camera Serial Interface
EVS
Enhanced Vision System
GPU
Graphic Processing Unit
ISP
Image Signal Processors
HUD
Head-up Display
SoC
System on a chip
SOM
System on a module
SWaP-C
Size, weight, power and cost
VI
Video Input
UAV
Unmanned Aerial Vehicle

\subsubsection{Abbreviations}
...

\subsection{References}
...

\subsection{Overview}
This project aims to create a device that is capable of combining images from two or more cameras in near-real time. Our proposed solution will use an Nvidia Jetson device, which we will use for its integrated graphics processing unit.

We need this graphics processor to combine the images from multiple cameras. The end goal is to have a system that uses multiple cameras that operate on different bands of the electromagnetic spectrum; infrared, ultraviolet, and visible light, among others. By using these varying bands, we should be able to produce an image that can be used to see in low-visibility situations, such as landing a plane in fog.

The images we produce will be 2D representations of our collective image captures. In other words, we do not aim to create a 3D image or a dynamic focus image. This is certainly possible when using multiple cameras, but we simply aim to use multiple cameras on different spectral bands to create one image of one subject that is the combination of all images captured by the cameras.


\section{Project Description}
...
\subsection{Product Perspective}
The system will consist of three parts: one Nvidia TX1/2, one CSI board, and at least two cameras. The cameras connect to the CSI board, which is connected to the Nvidia device. The Nvidia device is responsible for decoding the serial data retrieved by the CSI board from the cameras. The Nvidia device will then be used to execute the software for image processing and combining images from multiple cameras.

<DIAGRAM HERE>

\subsection{Production Functions}
The EVS will be able to capture images from different spectral bands to create the clearest possible image in situations where visible light does not provide enough clarity. These images will be relayed in near-real time so that it can be used as a video feed. One use case would be for a pilot to be able to see the ground when landing in low-visibility conditions.

Since the EVS will be able to provide near to all weather operations, the images from the system will be analyzed and combined in several modes. The mode of equivalent vision can display images shot by cameras directly in normal visibilities condition such as clear daytime. The mode of synthetic vision will display images from the channels provide thermal images of the landscape and various types of lighting, for example incandescent, halogen, and LED lights, etcetera. Then the view on the display device will be real images combined with light structure

\subsubsection{Constraints}
The system must operate in near-real time. In other words, the camera feed(s) must be processed quickly enough for the user to make snap decisions based on the feed. The Nvidia board should process each frame before the next one arrives to be processed. If we’re recording at 30 frames per second (fps), then each output frame should be processed in less than 1/30 of a second.

\subsubsection{Assumptions \& Dependencies}
LIST ITEMS
Software deployed on Nvidia TX1/2 with Nvidia Jetpack from Ubuntu machine
Adequate power supplies being used
Cameras being aimed at same subject; capturing mostly the same image
Each camera works independently of the system

\section{Specific Requirements}
\begin{enumerate}
	\item TX1/2
		\subitem Adequate power supply
		\subitem Nvidia Jetpack software + Ubuntu system to deploy it
	\item CSI Board
		\subitem Appropriate connectors
	\item Cameras
		\subitem Visible light
		\subitem Infrared
		\subitem UV
		\subitem ...
\end{enumerate}

<TIMELINE>

\end{document}
