\documentclass[letterpaper,10pt,serif,draftclsnofoot,onecolumn,compsoc,titlepage]{IEEEtran}

\usepackage{graphicx}                                        
\usepackage{amssymb}                                         
\usepackage{amsmath}                                         
\usepackage{amsthm}                                          
\usepackage{cite}
\usepackage{alltt}                                           
\usepackage{float}
\usepackage{color}
\usepackage[hyphens]{url}
\usepackage{pgfgantt}
\usepackage{rotating}
\usepackage{enumitem}
\usepackage{gensymb}
\usepackage[T1]{fontenc}

\usepackage{balance}
\usepackage[TABBOTCAP, tight]{subfigure}
\usepackage{enumitem}

\usepackage{geometry}
\geometry{margin=.75in}
\usepackage{hyperref}
\usepackage{breakurl}
%\usetikzlibrary{shapes, positioning, calc}
\usepackage{caption}
\usepackage{listings}
%\usepackage[utf8]{inputenc}
%pull in the necessary preamble matter for pygments output

%% The following metadata will show up in the PDF properties
\hypersetup{
   colorlinks = true,
   citecolor = black,
   linkcolor = black,
   urlcolor = black,
   breaklinks = true,
   pdfauthor = {Shu-Ping Chien, Brock Smedley, W Keith Striby Jr},
   pdfkeywords = {CS461 "Senior Project" Software Design Description},
   pdftitle = {CS461 Software Design Description},
   pdfsubject = {CS461 Software Design Description},
   pdfpagemode = UseNone
}

\parindent = 0.0 in
\parskip = 0.1 in
\title{Software Design Description: Multi-Camera, SoM Based, Real-Time Video Processing for UAS and VR/AR Applications}
\author{Area 51: Shu-Ping Chien, Brock Smedley, W Keith Stirby Jr \\ 01 December 2017 \\ CS461, Senior Software Engineering Project, Fall 2017}

\lstset{
  language=C++,
  basicstyle=\small,
  breaklines=true
  }

\begin{document}
\begin{titlepage}
\maketitle
\begin{abstract}

abstract words \\

\thispagestyle{empty}
\end{abstract}
\end{titlepage}

%\newpage
%\tableofcontents

\newpage

\section{Frontispiece}

\subsection{Date of issue and status}

01 December 2017, In-progress \\

\subsection{Issuing Organization}

Rockwell Collins, CS Capstone Group 51, Oregon State University \\

\subsection{Authorship}

Shu-Ping Chien, Brock Smedley, W Keith Stirby Jr \\

\section{Introduction}

\subsection{Purpose}

This document is a general description of design concepts that will be used for our 
multi-camera, SoM based, real-time video processing for UAS and VR/AR applications. 
It is a reference to guide the development of our product.  \\

\subsection{Scope}

Our product will receive input from multiple cameras and then provide a video output at 
near real-time. Our software will receive the pixel data from the cameras and format 
the pixel streams so that image processing can occur. Image processing will then stitch 
the multiple streams of pixels being received to create a combined output. The software 
will be flashed onto the NVIDIA TX1/2, which receives input from the carrier 
board that is connected to the cameras. \\

\subsection{Overview}

The development and design of our product requires: hardware interface and system 
architecture, receiving camera input and formatting for image processing, and image 
processing for video output. The structure of our document reflects these three areas 
of our software required for our product. \\ 

\newpage
\section{References}

\bibliographystyle{ieeetr}
\bibliography{SDD_Group51}

\subsection{Definitions, Acronyms, Abbreviations}

\subsubsection{Definitions}

\begin{tabular}{|l|p{11cm}|}
	\hline
	\textbf{Term} & \textbf{Definition}\\
	multiple cameras & At least two cameras, but a maximum of six cameras for 
	video input.\\
	\hline
	near real-time & Fast enough that a human could not notice the time 
	delay (lag) between \newline real life images and images displayed by the system.\\
	\hline
	NVIDIA TX1/2 & NVIDIA GPUs, the Jetson TX1 or the Jetson TX2.\\
	\hline
	GitHub repository & version control software for source code hosting and development\\
	\hline
	spectral bands & Electromagnetic frequency ranges; different 
	spectrums of light, including \newline but not limited to infrared 
	and visible light.\\
	\hline
	stitch(ed) (video) output & a composite image formed from multiple images\\
	\hline
\end{tabular}

\subsubsection{Acronyms}

\begin{tabular}{|l|l|}
	\hline
	\textbf{Acronym} & \textbf{Term}\\
	\hline
	CSI & Camera Serial Interface\\
	\hline
	GPIO & General-purpose Input/Output\\
	\hline
	GPU & Graphic Processing Unit\\
	\hline
	I2C & The Inter-integrated Circuit Protocol\\
	\hline
	ISP & Image Signal Processors\\
	\hline
	SoM & System-on-module\\
	\hline
	USB & Universal Serial Bus\\
	\hline
	VI & Video Input\\
	\hline
\end{tabular}

\newpage
\section{System Overview}  

\subsection{Identified Stakeholders and Design Concerns}

Rockwell Collins is the primary customer of this product. The company provides 
engineering products in the aviation industry for commercial and military customers. 
Rockwell Collins is the primary user of the product and it is contributing 
to the development of a system that will be used in UAS and VR/AR applications. \\

\subsection{Hardware Context}

The hardware used in our product will be the NVIDIA Jetson TX1/2 as our SoM, 
a carrier board, and cameras. The input from cameras will be transferred through 
carrier boards, and the TX1/2 will format and process the input to produce a 
video output. The software we're developing will be on the TX1/2. \\

\subsection{Software Context}

The software pieces in this project include: GStreamer for transforming video input 
from CSI for image processing, and OpenGL development environment for image processing 
to produce the video output. The pipeline feature in GStreamer will reduce 
cost on time and storage to help achieve near real-time image processing. The OpenGL 
will stitch input images from GStreamer and print the output to the display device. \\

\section{System Architecture}

\subsection{Interfaces}
The NVIDIA TX2 is an SoM with an ARM processor and an NVIDIA GPU. It includes several interfaces that can be used to exchange data with the module such as Ethernet, USB, I2C, and GPIO, among others. These connect to the TX2 via a 400-pin connector that attaches to a carrier board, which has a set of ports to interface with. These interfaces will be used in our implementation of the project for varying purposes.

\subsubsection{Ethernet}
Ethernet is essential for development. It will allow us to connect to the internet and will also allow us to control the system remotely using SSH. Ethernet will likely not be used in the final implementation, however. The final implementation will likely have an image flashed directly to its memory so that there is no need to download any new software or data. The carrier board we will be using in the final implementation may not have an Ethernet connection, but if it does not have it, we can attach the TX2 to the dev board that it came with, which has at least one connection for every interface that the TX2 supports.

\subsubsection{USB}
USB will be necessary for flashing the TX2 with a new image. This only needs to be done once at the start of development, and possibly again when a production image is ready to be flashed, but it is nonetheless required for development. USB can also be used for simple file transfers in cases where the internet is not available. USB will also not be present on the final product, but it should not be necessary at that stage; we will not be using USB for any system-critical functions. The TX2 can always be plugged into the dev board if changes to the system need to be made, and then re-attached to the final-product board with the new data on it.

\subsubsection{I2C \& GPIO}
I2C and GPIO will be used in later phases of the project which we may not actually work on. These interfaces can be used for connecting things like environmental sensors and control interfaces like buttons and knobs. Our portion of the project will focus solely on creating the camera system on the TX2 which can then be expanded upon by other teams.

\subsection{Architecture}
\subsubsection{Operating Systems}
The TX2 supports ARM Linux 4 operating systems, and the OS we’ll be using for this project is Linux4Tegra (L4T). L4T is an ubuntu variant that comes preloaded with a bundle of software that can be used to take advantage of the processing power that the TX2 has to offer. L4T is simply an Ubuntu ARM image with NVIDIA drivers pre-installed. Using L4T takes the guesswork out of developing the software environment so that more time can be spent developing the core solution.

\subsubsection{Deployment Software}
L4T will be flashed onto the TX2 system memory using NVIDIA Jetpack. Jetpack allows us to choose the operating system and software to be installed on the TX2. It runs on a separate Ubuntu host which is connected to the TX2 via USB and a router connecting both the machines to the internet, and can be downloaded from the NVIDIA’s website after creating an account. Ubuntu is not required to run Jetpack, but NVIDIA strongly recommends it, and Ubuntu is a very user-friendly and robust operating system, so it will suit our development needs perfectly.

\subsubsection{Development}
Once L4T is flashed to the TX2 from Jetpack, we can use the TX2 like a normal computer, using USB to connect a mouse \& keyboard, and HDMI to connect a monitor. Development can occur directly on the TX2 or remotely via SSH (Ethernet). We will likely use a combination of the two; sometimes it is required, for example when internet access is not available; sometimes it’s just more convenient. \\

We will also make use of a GitHub repository to keep our code, which can be accessed by the TX2 via the internet. This will allow each group member to work on different parts of the project simultaneously without interfering with other group members’ work. Most Linux variants come with a version of Git, but if L4T does not, then we can always install it using Ubuntu’s package manager. \\

\subsection{Making Camera Input Ready for Image Processing}

\subsubsection{Multimedia API}

The raw pixel data from the cameras will be sent through the CSI and must be converted 
to the BRG color space in the VI (Video Input) unit before being sent to the ISPs 
(Image Signal Processors). Application development can use either libargus API 
included in L4T or GStreamer plugin to prepare the raw pixel data in the VI unit, and 
therefore convert the data to a format that’s recognizable by the ISP. NVIDIA does not 
support V4L2 when using CSI cameras, but GStreamer does and therefore will be used in 
our software.  \\

\subsubsection{GStreamer}

GStreamer architecture utilizes pipelines to process media and connect the processing 
elements. An additional architecture GStreamer uses is plugin, which provides each 
processing element. \\

\paragraph{Installing GStreamer}

To install GStreamer 1.0 and plugins the following commands will be entered in the 
command line: \\

sudo add-apt-repository universe \\
sudo add-apt-repository multiverse \\
sudo apt-get update \\
sudo apt-get install gstreamer1.0-tools gstreamer1.0-alsa gstreamer1.0-plugins-base 
gstreamer1.0-plugins-good gstreamer1.0-plugins-bad gstreamer1.0-plugins-ugly 
gstreamer1.0-libav \\
sudo apt-get install libgstreamer1.0-dev libgstreamer-plugins-base1.0-dev 
libgstreamer-plugins-good1.0-dev libgstreamer-plugins-bad1.0-dev \\

\paragraph{GStreamer Plugins}

To display what a camera is capturing \texttt{nvcamerasrc} included in the piping, 
and is a plugin that allows options for our software to control ISP properties. \\

For converting video frames from the CSI camera the \texttt{videoconvert} plugin will 
be included in piping, along with \texttt{video/x-raw, format=(string){}} to specify 
details on the conversion input and output. \\

\subsection{Image Processing for Video Output}
Since the  NVIDIA Jetpack includes OpenGL 4.3, 4.4, and 4.5, the program will use OpenGL and the OpenGL Shading Language for image processing to detect edge and draw on images, and the output will be sent to a display device. \\

\subsubsection{OpenGL Environment Setup}
Before drawing anything with OpenGL, the OpenGL environment need to be set up at first in order to specify the size of screen window and viewing features. Then the output images will be generated in display function, where the program does image processing and draws to. \\

\subsubsection{Bind Video Input as Textures}
The video output will be read as textures and processed in the shaders. The OpenGL binds a texture with the following code:\\

\begin{tabular}{|p{15cm}|}
\begin{lstlisting}
GLuint tex;
glGenTextures(1, &tex);
glBindTexture(GL_TEXTURE_2D, tex);
glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_REPEAT);
glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T,GL_REPEAT);
glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR);
glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR);
glTexImage2D(GL_TEXTURE_2D, 0, GL_RGB, width, height, 0, GL_RGB, GL_UNSIGNED_BYTE, image);
\end{lstlisting}
\end{tabular}

Where the image is the a loaded texture image from video output, and it is composed of 2D arrays of pixels. Then the texture can be used in the OpenGL Shading Language with the created shaders.\\

There are two shaders used in this project, vertex shader and fragment shader. The vertex shader needs to be modified so that the texture coordinates can be interpolated over the fragments, and with the output from the vertex shader, the fragment shader fused images as output. \\

\subsubsection{Vertex Shader}
Since textures are sampled using texture coordinates, which are represented as s and t, the vertex shader uses the input to modify the texture coordinates and pass to fragment shader as output. In order to provide access to the texture in the fragment shader, the program will set the texture as uniform variable of type sampler2D in the fragment shader.\\

\subsubsection{Fragment Shader}
The fragment shader renders an image by filling color in pixels. The color filled in a pixel will be determined based on the texture, where the program read the RGB information with output of texture coordinate from vertex shader.\\

In OpenGL:\\
\begin{tabular}{|p{15cm}|}
\begin{lstlisting}
glUniform1i(glGetUniformLocation(shaderProgram, "texName"), 0);
\end{lstlisting}
\end{tabular}
\\
\\
In Fragment Shader:\\
\begin{tabular}{|p{15cm}|}
\begin{lstlisting}
   in vec2 Texcoord;    //from vertex shader
   out vec4 outColor;
   uniform sampler2D texName;

   void main()
   {
         outColor = texture(tex, Texcoord);
   }
\end{lstlisting}
\end{tabular}

In this code in the fragment shader, the output, outColor, will be the color from function texture(tex, Texcoord). By controlling the Texcoord, the fragment shader can draw on specific coordinate to generate an output image.\\

\nocite{*}
%\newpage
%\bibliographystyle{ieeetr}
%\bibliography{SDD_Group51}
\end{document}